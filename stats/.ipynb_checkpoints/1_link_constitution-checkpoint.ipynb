{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries I used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import httplib\n",
    "import urllib2\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from httplib import IncompleteRead\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load initial websites from which you want to expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_web=pd.read_csv(\"data/site_web.csv\",sep=\",\",encoding=\"utf-8\")\n",
    "site_web= list(site_web.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_page=[]\n",
    "for i,s in enumerate(site_web):\n",
    "    print i,s\n",
    "    try:\n",
    "        req = urllib2.Request(s.decode('utf-8'), headers={'User-Agent' : \"Magic Browser\"}) \n",
    "        page = urllib2.urlopen( req )\n",
    "        soup = BeautifulSoup(page)\n",
    "    except urllib2.HTTPError,e:\n",
    "        continue\n",
    "    except urllib2.URLError, e:\n",
    "        continue\n",
    "    except httplib.HTTPException, e:\n",
    "        continue\n",
    "    except Exception:\n",
    "        continue\n",
    "    #find all links\n",
    "    all_links= soup.find_all(\"a\")\n",
    "    #ensure all links are valid\n",
    "    link_inside= [link.get(\"href\") for link in all_links if link.get(\"href\") is not None]\n",
    "    link_inside= [link if link.startswith('http') or link.startswith('www') else s + link for link in link_inside]\n",
    "    #do not keep links from starter links\n",
    "    link_inside = [link for link in link_inside if link not in site_web]\n",
    "    #do not keep links that have been already recorded from the current loop\n",
    "    link_inside = [link for link in link_inside if link not in all_page]\n",
    "    for k in np.unique(link_inside):\n",
    "        print k\n",
    "        all_page.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_page=np.unique(all_page)\n",
    "len(all_page)\n",
    "df = pd.DataFrame(data=all_page, index= range(len(all_page)), columns=['link'])\n",
    "df.to_csv(\"data/all_page.csv\",sep=\";\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2463, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
